{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd64df5",
   "metadata": {},
   "source": [
    "# ResNetHE - Metastasis Classification from Histopathology Images\n",
    "\n",
    "This notebook trains and evaluates a DenseNet-based model for binary classification of metastasis in histopathology image tiles.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load and preprocess image tiles with corresponding labels\n",
    "2. Split data by patient (no patient overlap between train/val)\n",
    "3. Train DenseNet with transfer learning\n",
    "4. Evaluate on validation set (patch-level and patient-level)\n",
    "5. Generate predictions and aggregate by patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e0ad6",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fcd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4180e8",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Paths\n",
    "IMAGE_DIR = \"Images/2048Tiles\"\n",
    "CSV_PATH = \"CSV/Image_Data.csv\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = ['0', '1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf93b20",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetastasisDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading histopathology image tiles with metastasis labels.\n",
    "    \n",
    "    Matches image filenames to patient codes in the CSV file and assigns\n",
    "    corresponding metastasis labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    unmatched_labels = []  # Track unmatched images globally for debugging\n",
    "    \n",
    "    def __init__(self, image_dir, csv_path, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load and process label CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Code'] = df['Code'].astype(str).str.replace(r'[-_]', '', regex=True)\n",
    "        self.label_map = dict(zip(df['Code'], df['Metastasis'].astype(str)))\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.patient_ids = []\n",
    "        unmatched_images = 0\n",
    "        \n",
    "        # Match images to labels\n",
    "        for fname in os.listdir(image_dir):\n",
    "            fname_norm = fname.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            matched = False\n",
    "            \n",
    "            for pid in self.label_map:\n",
    "                pid_norm = pid.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "                if pid_norm in fname_norm:\n",
    "                    self.image_paths.append(os.path.join(image_dir, fname))\n",
    "                    self.labels.append(int(self.label_map[pid]))\n",
    "                    self.patient_ids.append(pid)\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                unmatched_images += 1\n",
    "                MetastasisDataset.unmatched_labels.append(fname)\n",
    "        \n",
    "        print(f'Number of images not matched: {unmatched_images}')\n",
    "        print(f\"Loaded {len(self.image_paths)} labeled images.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, patient_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d3377",
   "metadata": {},
   "source": [
    "## 4. Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62509195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07493e1",
   "metadata": {},
   "source": [
    "## 5. Data Loading with Patient-Level Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3333f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_split_loaders(image_dir, csv_path, batch_size, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/val DataLoaders with patient-level splitting.\n",
    "    \n",
    "    Ensures no patient appears in both training and validation sets,\n",
    "    which is critical for valid evaluation in medical imaging.\n",
    "    \"\"\"\n",
    "    # Load full dataset to get patient information\n",
    "    full_dataset = MetastasisDataset(\n",
    "        image_dir=image_dir,\n",
    "        csv_path=csv_path,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame for patient-level operations\n",
    "    df = pd.DataFrame({\n",
    "        \"image_path\": full_dataset.image_paths,\n",
    "        \"label\": full_dataset.labels,\n",
    "        \"patient_id\": full_dataset.patient_ids\n",
    "    })\n",
    "    \n",
    "    # Get patient-level labels (max ensures any positive patch = positive patient)\n",
    "    patient_labels = df.groupby(\"patient_id\")[\"label\"].max().reset_index()\n",
    "    \n",
    "    # Split at patient level\n",
    "    train_patients, val_patients = train_test_split(\n",
    "        patient_labels[\"patient_id\"],\n",
    "        test_size=test_size,\n",
    "        stratify=patient_labels[\"label\"],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Filter images by patient split\n",
    "    train_df = df[df[\"patient_id\"].isin(train_patients)]\n",
    "    val_df = df[df[\"patient_id\"].isin(val_patients)]\n",
    "    \n",
    "    # Sanity checks\n",
    "    print(f\"\\nPatient-level split summary:\")\n",
    "    print(f\"  Train patients: {train_df['patient_id'].nunique()}\")\n",
    "    print(f\"  Val patients: {val_df['patient_id'].nunique()}\")\n",
    "    print(f\"  Overlap: {set(train_df['patient_id']) & set(val_df['patient_id'])}\")\n",
    "    print(f\"\\nTrain label distribution:\\n{train_df['label'].value_counts().to_string()}\")\n",
    "    print(f\"\\nVal label distribution:\\n{val_df['label'].value_counts().to_string()}\")\n",
    "    \n",
    "    # Create train dataset\n",
    "    train_dataset = MetastasisDataset(\n",
    "        image_dir=image_dir,\n",
    "        csv_path=csv_path,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset.image_paths = train_df[\"image_path\"].tolist()\n",
    "    train_dataset.labels = train_df[\"label\"].tolist()\n",
    "    train_dataset.patient_ids = train_df[\"patient_id\"].tolist()\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = MetastasisDataset(\n",
    "        image_dir=image_dir,\n",
    "        csv_path=csv_path,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset.image_paths = val_df[\"image_path\"].tolist()\n",
    "    val_dataset.labels = val_df[\"label\"].tolist()\n",
    "    val_dataset.patient_ids = val_df[\"patient_id\"].tolist()\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def check_class_balance(train_dataset, val_dataset):\n",
    "    \"\"\"Print class balance statistics for train and validation sets.\"\"\"\n",
    "    train_counts = collections.Counter(train_dataset.labels)\n",
    "    val_counts = collections.Counter(val_dataset.labels)\n",
    "    \n",
    "    train_ratio = train_counts[1] / (train_counts[0] + train_counts[1])\n",
    "    val_ratio = val_counts[1] / (val_counts[0] + val_counts[1])\n",
    "    \n",
    "    print(\"\\nClass Balance Analysis:\")\n",
    "    print(f\"  Train - Metastasis: {train_counts[1]}, No Metastasis: {train_counts[0]}\")\n",
    "    print(f\"  Train - Metastasis ratio: {train_ratio:.3f}\")\n",
    "    print(f\"  Val - Metastasis: {val_counts[1]}, No Metastasis: {val_counts[0]}\")\n",
    "    print(f\"  Val - Metastasis ratio: {val_ratio:.3f}\")\n",
    "    \n",
    "    if abs(train_ratio - val_ratio) < 0.03:\n",
    "        print(\"  ✓ Training and validation sets are balanced\")\n",
    "    else:\n",
    "        print(\"  ⚠ Training and validation sets have different class distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255c5ff",
   "metadata": {},
   "source": [
    "## 6. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet_option(num=201):\n",
    "    \"\"\"Get a pretrained DenseNet model.\"\"\"\n",
    "    options = {\n",
    "        121: (models.densenet121, models.DenseNet121_Weights.DEFAULT, \"DenseNet121\"),\n",
    "        169: (models.densenet169, models.DenseNet169_Weights.DEFAULT, \"DenseNet169\"),\n",
    "        201: (models.densenet201, models.DenseNet201_Weights.DEFAULT, \"DenseNet201\"),\n",
    "    }\n",
    "    \n",
    "    if num not in options:\n",
    "        raise ValueError(f\"Invalid DenseNet variant: {num}. Choose from {list(options.keys())}\")\n",
    "    \n",
    "    model_fn, weights, name = options[num]\n",
    "    return model_fn(weights=weights), name\n",
    "\n",
    "\n",
    "def freeze_all_except(model, train_blocks=None, train_classifier=True):\n",
    "    \"\"\"\n",
    "    Freeze model layers except specified DenseNet blocks and classifier.\n",
    "    \n",
    "    Args:\n",
    "        model: DenseNet model\n",
    "        train_blocks (list): List of block indices to train (e.g., [4] or [3, 4])\n",
    "        train_classifier (bool): Whether to train the classifier layer\n",
    "    \"\"\"\n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze classifier\n",
    "    if train_classifier:\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze specified DenseNet blocks\n",
    "    if train_blocks is not None:\n",
    "        for block_num in train_blocks:\n",
    "            block_name = f\"denseblock{block_num}\"\n",
    "            block = getattr(model.features, block_name, None)\n",
    "            if block is not None:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                print(f\"[Warning] {block_name} not found in model\")\n",
    "\n",
    "\n",
    "def create_model(densenet_variant=201, train_blocks=[4], num_classes=NUM_CLASSES):\n",
    "    \"\"\"Create and configure a DenseNet model for metastasis classification.\"\"\"\n",
    "    # Load pretrained model\n",
    "    model, model_name = densenet_option(densenet_variant)\n",
    "    \n",
    "    # Replace classifier with custom head\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Apply transfer learning configuration\n",
    "    freeze_all_except(model, train_blocks=train_blocks, train_classifier=True)\n",
    "    \n",
    "    # Move to device\n",
    "    model.to(device)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\n{model_name} configured:\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Training blocks: {train_blocks}\")\n",
    "    \n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b94f3f",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ac2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, cm, report, auroc, auprc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 30)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, cm, report, val_auroc, val_auprc = evaluate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_test_set(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set and print comprehensive metrics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL TEST SET EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_acc, cm, report, auroc, auprc = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f'\\nTest Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    print(f'Test AUROC: {auroc:.4f}')\n",
    "    print(f'Test AUPRC: {auprc:.4f}')\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    print(f\"\\nClassification Report:\\n{report}\")\n",
    "    \n",
    "    return test_loss, test_acc, auroc, auprc\n",
    "\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"Plot training and validation loss/accuracy curves.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(history['train_acc'], label='Train Acc')\n",
    "    ax2.plot(history['val_acc'], label='Val Acc')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43522a7b",
   "metadata": {},
   "source": [
    "## 8. Stain-Specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stain_loader(original_loader, stain_name):\n",
    "    \"\"\"\n",
    "    Create a DataLoader containing only images of a specified stain type.\n",
    "    \n",
    "    Args:\n",
    "        original_loader: Existing DataLoader\n",
    "        stain_name (str): Stain type to filter for (e.g., 'HE', 'MITF')\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader: Filtered DataLoader for the specified stain\n",
    "    \"\"\"\n",
    "    dataset = original_loader.dataset\n",
    "    \n",
    "    # Find indices matching the stain\n",
    "    stain_indices = []\n",
    "    for idx, path in enumerate(dataset.image_paths):\n",
    "        filename = os.path.basename(path)\n",
    "        if stain_name.upper() in filename.upper():\n",
    "            stain_indices.append(idx)\n",
    "    \n",
    "    print(f\"{stain_name}: {len(stain_indices)} images found\")\n",
    "    \n",
    "    # Create subset and loader\n",
    "    subset = Subset(dataset, stain_indices)\n",
    "    stain_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=original_loader.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return stain_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d566963",
   "metadata": {},
   "source": [
    "## 9. Prediction Export and Patient Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patch_predictions_to_csv(model, dataloader, device, output_csv='patch_level_predictions.csv'):\n",
    "    \"\"\"\n",
    "    Save individual patch predictions to CSV file.\n",
    "    Handles both regular Datasets and Subsets (for stain-specific evaluation).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    csv_data = []\n",
    "    \n",
    "    # Handle Subset vs regular Dataset\n",
    "    dataset = dataloader.dataset\n",
    "    if hasattr(dataset, 'dataset'):\n",
    "        original_dataset = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "    else:\n",
    "        original_dataset = dataset\n",
    "        indices = range(len(dataset))\n",
    "    \n",
    "    print(\"Computing patch-level predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels, patient_ids) in enumerate(tqdm(dataloader, desc=\"Processing patches\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                sample_idx = batch_idx * dataloader.batch_size + i\n",
    "                if sample_idx < len(indices):\n",
    "                    actual_idx = indices[sample_idx]\n",
    "                    img_path = original_dataset.image_paths[actual_idx]\n",
    "                    filename = os.path.basename(img_path)\n",
    "                    \n",
    "                    csv_data.append({\n",
    "                        'filename': filename,\n",
    "                        'true_label': labels[i].item(),\n",
    "                        'predicted_class': predicted[i].item(),\n",
    "                        'prob_no_metastasis': probs[i, 0].item(),\n",
    "                        'prob_metastasis': probs[i, 1].item(),\n",
    "                        'confidence': probs[i, predicted[i]].item(),\n",
    "                        'correct': int(predicted[i].item() == labels[i].item())\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved {len(df)} patch predictions to: {output_csv}\")\n",
    "    print(f\"\\nCSV Preview:\\n{df.head(10).to_string(index=False)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_by_patient_robust(df, trim_chars=3, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Aggregate patch-level predictions to patient-level using mean probability pooling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle null/empty filenames\n",
    "    df = df[df['filename'].notna() & (df['filename'] != '')]\n",
    "    \n",
    "    # Extract patient ID\n",
    "    if trim_chars > 0:\n",
    "        too_short = df['filename'].str.len() <= trim_chars\n",
    "        if too_short.any():\n",
    "            print(f\"⚠️  WARNING: {too_short.sum()} filenames are too short, skipping trim\")\n",
    "            df['patient_id'] = df.apply(\n",
    "                lambda row: row['filename'][:-trim_chars] if len(row['filename']) > trim_chars else row['filename'],\n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            df['patient_id'] = df['filename'].str[:-trim_chars]\n",
    "    else:\n",
    "        df['patient_id'] = df['filename']\n",
    "    \n",
    "    df['patient_id'] = df['patient_id'].str.strip()\n",
    "    \n",
    "    if not case_sensitive:\n",
    "        df['patient_id'] = df['patient_id'].str.lower()\n",
    "    \n",
    "    # Check for label inconsistencies\n",
    "    label_check = df.groupby('patient_id')['true_label'].nunique()\n",
    "    inconsistent = label_check[label_check > 1]\n",
    "    if len(inconsistent) > 0:\n",
    "        print(f\"⚠️  WARNING: {len(inconsistent)} patients have inconsistent labels!\")\n",
    "        for pid in inconsistent.index:\n",
    "            labels = df[df['patient_id'] == pid]['true_label'].unique()\n",
    "            print(f\"   {pid}: {labels}\")\n",
    "    \n",
    "    # Aggregate by patient\n",
    "    aggregated = df.groupby('patient_id').agg({\n",
    "        'true_label': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "        'prob_no_metastasis': 'mean',\n",
    "        'prob_metastasis': 'mean',\n",
    "        'filename': ['count', 'first']\n",
    "    }).reset_index()\n",
    "    \n",
    "    aggregated.columns = [\n",
    "        'patient_id', 'true_label', 'prob_no_metastasis',\n",
    "        'prob_metastasis', 'num_patches', 'sample_filename'\n",
    "    ]\n",
    "    \n",
    "    # Compute predictions and metrics\n",
    "    aggregated['predicted_class'] = (\n",
    "        aggregated['prob_metastasis'] > aggregated['prob_no_metastasis']\n",
    "    ).astype(int)\n",
    "    aggregated['confidence'] = aggregated[['prob_no_metastasis', 'prob_metastasis']].max(axis=1)\n",
    "    aggregated['correct'] = (aggregated['predicted_class'] == aggregated['true_label']).astype(int)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n✓ Aggregated {len(df)} patches into {len(aggregated)} patients\")\n",
    "    print(f\"  Avg patches/patient: {aggregated['num_patches'].mean():.1f}\")\n",
    "    print(f\"  Patient-level accuracy: {aggregated['correct'].mean() * 100:.2f}%\")\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def aggregate_by_patient_comprehensive(df, patient_id_length=-8):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by patient with comprehensive metrics and reporting.\n",
    "    \"\"\"\n",
    "    df['patient_id'] = df['filename'].str[:patient_id_length]\n",
    "    \n",
    "    # Check for label consistency\n",
    "    label_check = df.groupby('patient_id')['true_label'].nunique()\n",
    "    inconsistent = label_check[label_check > 1]\n",
    "    if len(inconsistent) > 0:\n",
    "        print(f\"WARNING: {len(inconsistent)} patients have inconsistent labels!\")\n",
    "        print(inconsistent)\n",
    "    \n",
    "    # Aggregate\n",
    "    aggregated = df.groupby('patient_id').agg({\n",
    "        'true_label': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "        'prob_no_metastasis': 'mean',\n",
    "        'prob_metastasis': 'mean',\n",
    "        'predicted_class': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    aggregated.rename(columns={'predicted_class': 'num_patches'}, inplace=True)\n",
    "    aggregated['predicted_class'] = (\n",
    "        aggregated['prob_metastasis'] > aggregated['prob_no_metastasis']\n",
    "    ).astype(int)\n",
    "    aggregated['confidence'] = aggregated[['prob_no_metastasis', 'prob_metastasis']].max(axis=1)\n",
    "    aggregated['correct'] = (aggregated['predicted_class'] == aggregated['true_label']).astype(int)\n",
    "    \n",
    "    # Reorder columns\n",
    "    aggregated = aggregated[[\n",
    "        'patient_id', 'num_patches', 'true_label', 'predicted_class',\n",
    "        'prob_no_metastasis', 'prob_metastasis', 'confidence', 'correct'\n",
    "    ]]\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    accuracy = aggregated['correct'].mean() * 100\n",
    "    auroc = roc_auc_score(aggregated['true_label'], aggregated['prob_metastasis'])\n",
    "    cm = confusion_matrix(aggregated['true_label'], aggregated['predicted_class'])\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"PATIENT-LEVEL AGGREGATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total patches: {len(df)}\")\n",
    "    print(f\"Total patients: {len(aggregated)}\")\n",
    "    print(f\"Avg patches/patient: {aggregated['num_patches'].mean():.1f} \"\n",
    "          f\"(min: {aggregated['num_patches'].min()}, max: {aggregated['num_patches'].max()})\")\n",
    "    print(f\"\\nPatient-level accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Patient-level AUROC: {auroc:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                 Predicted\")\n",
    "    print(f\"               No Met  Metastasis\")\n",
    "    print(f\"True No Met      {cm[0,0]:4d}    {cm[0,1]:4d}\")\n",
    "    print(f\"True Metastasis  {cm[1,0]:4d}    {cm[1,1]:4d}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        aggregated['true_label'],\n",
    "        aggregated['predicted_class'],\n",
    "        target_names=['No Metastasis', 'Metastasis'],\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8002343",
   "metadata": {},
   "source": [
    "## 10. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d731ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_dataset, val_dataset = create_patient_split_loaders(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    csv_path=CSV_PATH,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "check_class_balance(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba6a59",
   "metadata": {},
   "source": [
    "## 11. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose DenseNet variant: 121, 169, or 201\n",
    "# Choose blocks to train: [4], [3,4], [2,3,4], or [] for classifier only\n",
    "model, model_name = create_model(\n",
    "    densenet_variant=201,\n",
    "    train_blocks=[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e05e0",
   "metadata": {},
   "source": [
    "## 12. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "plot_training_history(history, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8466c18",
   "metadata": {},
   "source": [
    "## 13. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_auroc, test_auprc = evaluate_test_set(\n",
    "    model, val_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636957b",
   "metadata": {},
   "source": [
    "## 14. Stain-Specific Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stain-specific loaders\n",
    "val_loader_he = get_stain_loader(val_loader, 'HE')\n",
    "val_loader_mitf = get_stain_loader(val_loader, 'MITF')\n",
    "val_loader_anx = get_stain_loader(val_loader, 'ANX')\n",
    "val_loader_bcl2 = get_stain_loader(val_loader, 'BCL2')\n",
    "val_loader_bcl3 = get_stain_loader(val_loader, 'BCL3')\n",
    "val_loader_pbp = get_stain_loader(val_loader, 'PBP')\n",
    "val_loader_pir = get_stain_loader(val_loader, 'PIR')\n",
    "\n",
    "# Evaluate on a specific stain (example: ANX)\n",
    "test_loss, test_acc, test_auroc, test_auprc = evaluate_test_set(model, val_loader_anx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a457f",
   "metadata": {},
   "source": [
    "## 15. Save Predictions and Aggregate by Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b97021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save patch-level predictions\n",
    "patch_df = save_patch_predictions_to_csv(\n",
    "    model=model,\n",
    "    dataloader=val_loader,\n",
    "    device=device,\n",
    "    output_csv='patch_level_predictions.csv'\n",
    ")\n",
    "\n",
    "# Aggregate to patient level\n",
    "patient_df = aggregate_by_patient_comprehensive(patch_df, patient_id_length=11)\n",
    "patient_df.to_csv('patient_level_predictions.csv', index=False)\n",
    "print(\"✓ Saved patient-level predictions\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
